## Big O

- In week 0, we saw different types of algorithms and their running times: ![chart with: "size of problem" as x–axis; "time to solve" as y–axis; red, steep straight line from origin to top of graph labeled "n"; yellow, less steep straight line from origin to top of graph labeled "n/2"; green, curved line that gets less and less steep from origin to right of graph labeled "log_2 n"](https://cs50.harvard.edu/x/2020/notes/3/running_time.png)
- The more formal way to describe this is with big _O_ notation, which we can think of as “on the order of”. For example, if our algorithm is linear search, it will take approximately _O_(_n_) steps, “on the order of _n_”. In fact, even an algorithm that looks at two items at a time and takes _n_/2 steps has _O_(_n_). This is because, as _n_ gets bigger and bigger, only the largest term, _n_, matters.
- Similarly, a logarithmic running time is _O_(log _n_), no matter what the base is, since this is just an approximation of what happens with _n_ is very large.
- There are some common running times:
  - _O_(\_n_2)
  - _O_(_n_ log _n_)
  - _O_(_n_)
    - (linear search)
  - _O_(log _n_)
    - (binary search)
  - _O_(1)
- Computer scientists might also use big Ω, big Omega notation, which is the lower bound of number of steps for our algorithm. (Big _O_ is the upper bound of number of steps, or the worst case, and typically what we care about more.) With linear search, for example, the worst case is _n_ steps, but the best case is 1 step since our item might happen to be the first item we check. The best case for binary search, too, is 1 since our item might be in the middle of the array.
- And we have a similar set of the most common big Ω running times:
  - Ω(\_n_2)
  - Ω(_n_ log _n_)
  - Ω(_n_)
    - (counting the number of items)
  - Ω(log _n_)
  - Ω(1)
    - (linear search, binary search)
