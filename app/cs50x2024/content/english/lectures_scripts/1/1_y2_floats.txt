In fact, let me go ahead and do this. Let me write a program that I'm going to go ahead and call float.c. And this is just going to be a program that gets a couple of floating point values from the user. Let me go ahead and include the CS50 library. Let's go ahead and include stdio.h, int main void, as before. And all I want to do here with this program is get a couple of floats. So give me a float. We'll call it x. Get_float, and I'll prompt the human for x. Then let me go ahead and get another. I'll call it y. Get_float, quote unquote y. And recall that a float is just a number that has a decimal point in it, a so-called real number. Now let's just do some simple division. I claim that computers can do addition, subtraction, and so forth. So let's do that. Let's just tell it that x divided by y is going to equal the following-- percent f backslash n x divided by y semicolon. So that's just sort of a very simplistic calculator that I've implemented that only supports division. Let me go ahead and compile this by going and typing make floats. And you'll see that it did compile. So floats with dot slash. x is going to be, say, 1. y is going to be 10. OK, viola. x divided by y equals 0.10000. That's pretty nice. And recall, if you don't want to see all those zeros, you can just say, show me one decimal point by adding 0.1. Recompile and then rerun it. And now do 1, 10. OK, so now it's 1/10. Or is it? Now that I have this ability to look past the decimal point, why don't I look not a few places or one place. Let me go ahead and look maybe 10 places after the decimal point. Let me rerun this as make floats, ./floats, 1, 10. Interesting. All right, that seems a little strange. Maybe it was just a fluke. Let's look out a little further. Let's look 50 decimal places out. Let's go ahead and recompile this. And it turns out, there's some keyboard shortcuts. I'm now hitting up and down on my keyboard, which will scroll through your entire history of commands so you don't have to remember everything. So to save time, I'm now just going up and down. Let me go ahead and do ./floats now, 1, 10. Oh my god, division is a lie. So when your grade school teachers or whatnot taught you that 1 divided by 10 is 1/10, or 0.10000 infinitely, apparently that's not true. According to this computer, 1/10 is actually this value. So how do we reconcile that? Who is right, grade school math or computers? And what might explain? Any thoughts? Yeah? AUDIENCE: It only stores so much so then half of that, you don't know what's going on over there. DAVID MALAN: Yeah, that's a good way of putting it. Computers can only store so much, so after a certain point, you don't know what's going on out there. I like that. Because that's indeed true. If you only have a finite amount of hardware, like a finite amount of memory, at some point, the computer has to decide, I can count no higher than this value. Or I can store no more than this many numbers after the decimal point. You might be using 32 bits, which a float is. You could use more bits, like a double, as I described it earlier, literally uses twice as many bits, 64 bits. So that means we could get farther out before we see that imprecision. But you will see it. Computers are, indeed, not perfect in this sense. They can only store a finite amount of information. And so in that sense, the computer is storing the closest possible match for 1 divided by 10 that it can. Because you can't possibly store an infinite number of numbers 100% precisely using a finite amount of information. And we see this in another context, too.