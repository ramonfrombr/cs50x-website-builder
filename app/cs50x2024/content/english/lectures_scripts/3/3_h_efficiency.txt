So how might we now think of this? Well, just as in week 0, we had a picture like this. And we claimed that these algorithms were either linear in nature, literally a straight line, like Eric's, or a little more curved or logarithmic, so to speak, like [? Nizari's. ?] And these had fundamentally different shapes. And we refer to them really by the number of steps they might take in the worst case. 

If the phone book or today, the number of lockers was n in total, it might take as many as n steps for Eric or anyone to find Mike Smith or the number 50 from left to right. If in week 0, I did two pages at a time, you can actually speed that up, but the shape of the line was the same. Eric didn't do that here, but he could have. With two hands, he maybe could have looked at two lockers at once. So that might have been an intermediate step between those two extremes. But logarithmic was this more curved shape. 

But today, we're going to start to formalize this a little bit so that we don't keep talking about searching and binary search in linear search alone, but other algorithms as well. And computer scientists now actually have terminology with which to describe algorithms and just how well designed your algorithm is or how well implemented your code is. And it's generally called big O, literally a capital, italicized O. 

Big O notation just means on the order of. So if you were asked by someone what is the efficiency of your algorithm or the efficiency of your code, you could kind of wave your hand, literally and figuratively, and give them an approximation of just how fast or slow your code is. So instead of saying literally n steps or n/2 or log n steps, a computer scientists would typically say, ah, that algorithm is on the order of n or on the order of n/2 or on the order of log n. 

So this is just cryptic-looking syntax that you pronounce verbally as "on the order of." And it's kind of written like a math function, just as we have here. But it turns out that when you're using big O notation, it really is kind of hand-waving. Like, it's just meant to be an approximation. 

And you know what? In this case here, these lines are so similar looking, I'm actually going to throw away the divided by 2. And we'll see why this is OK in just a moment. But those are so similar that I'm just going to call them the same thing. And it turns out-- and it's fine if you don't recall logarithms too well-- the base 2 there it doesn't really matter. I'm going to throw that away. 

It can be base 2 or 3 or 10. They're all within multiples of one another. So that's no big deal either, I claim. And if you don't recall, that's OK, too. 

But the reason I claim that this red line and this yellow line are essentially the same thing is because if the problem gets big enough, that is the size of the problem gets bigger and bigger, and I only have so much screen here-- so let me instead just zoom out so that we see more y-axis and more x-axis. Notice how much closer the yellow and red lines even get to one another. And honestly, if I kept zooming out so that we could see bigger and bigger and bigger problems, these, frankly, would look pretty much the same. 

So when a computer scientist describes the efficiency of an algorithm, they say it's on the order of n, even if it's technically on the order of n/2. And here, too, on the order of [? law, ?] [? again ?] irrespective of what the base is. So it's kind of nice, right? Even though it looks a little mathy, you can still kind of wave your hand and approximate just a little bit. 