- The running time of searching a tree is _O_(log _n_), and inserting nodes while keeping the tree balanced is also _O_(log _n_). By spending a bit more memory and time to maintain the tree, we’ve now gained faster searching compared to a plain linked list.
- A data structure with almost a constant time search is a **hash table**, which is a combination of an array and a linked list. We have an array of linked lists, and each linked list in the array has elements of a certain category. For example, in the real world we might have lots of nametags, and we might sort them into 26 buckets, one labeled with each letter of the alphabet, so we can find nametags by looking in just one bucket.
- We can implement this in a hash table with an array of 26 pointers, each of which points to a linked list for a letter of the alphabet:  
  ![vertical array with 26 boxes, the first with an arrow pointing to a box labeled Albus, the second empty, the third with an arrow pointing to a box labeled Cedric ... the seventh with an arrow pointing to a box labeled Ginny with an arrow from that box pointing to a box labeled George...](https://cs50.harvard.edu/x/2020/notes/5/hash_table.png)
- Since we have random access with arrays, we can add elements quickly, and also index quickly into a bucket.
- A bucket might have multiple matching values, so we’ll use a linked list to store all of them horizontally. (We call this a collision, when two values match in some way.)
- This is called a hash table because we use a hash function, which takes some input and maps it to a bucket it should go in. In our example, the hash function is just looking at the first letter of the name, so it might return `0` for “Albus” and `25` for “Zacharias”.
- But in the worst case, all the names might start with the same letter, so we might end up with the equivalent of a single linked list again. We might look at the first two letters, and allocate enough buckets for 26\*26 possible hashed values, or even the first three letters, and now we’ll need 26\*26\*26 buckets. But we could still have a worst case where all our values start with the same three characters, so the running time for search is _O_(_n_). In practice, though, we can get closer to _O_(1) if we have about as many buckets as possible values, especially if we have an ideal hash function, where we can sort our inputs into unique buckets.
- We can use another data structure called a **trie** (pronounced like “try”, and is short for “retrieval”):  
  ![array with letters from A-Z in 26 elements, with H pointing to another array with all 26 letters. this array's A and E each point to two more arrays of all 26 letters, and this continues in a tree until the bottom-most arrays have only one letter marked as valid](https://cs50.harvard.edu/x/2020/notes/5/trie.png)
  - Imagine we want to store a dictionary of words efficiently, and be able to access each one in constant time. A trie is like a tree, but each node is an array. Each array will have each letter, A-Z, stored. For each word, the first letter will point to an array, where the next valid letter will point to another array, and so on, until we reach something indicating the end of a valid word. If our word isn’t in the trie, then one of the arrays won’t have a pointer or terminating character for our word. Now, even if our data structure has lots of words, the lookup time will be just the length of the word we’re looking for, and this might be a fixed maximum so we have _O_(1) for searching and insertion. The cost for this, though, is 26 times as much memory as we need for each character.
- There are even higher-level constructs, **abstract data structures**, where we use our building blocks of arrays, linked lists, hash tables, and tries to implement a solution to some problem.
- For example, one abstract data structure is a **queue**, where we want to be able to add values and remove values in a first-in-first-out (FIFO) way. To add a value we might enqueue it, and to remove a value we would dequeue it. And we can implement this with an array that we resize as we add items, or a linked list where we append values to the end.
- An “opposite” data structure would be a **stack**, where items most recently added (pushed) are removed (popped) first, in a last-in-first-out (LIFO) way. Our email inbox is a stack, where our most recent emails are at the top.
- Another example is a **dictionary**, where we can map keys to values, or strings to values, and we can implement one with a hash table where a word comes with some other information (like its definition or meaning).
- We take a look at [“Jack Learns the Facts About Queues and Stacks”](https://www.youtube.com/watch?v=2wM6_PuBIxY), an animation about these data structures.
